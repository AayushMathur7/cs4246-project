{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scraping and Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gnews\n",
    "!pip install newspaper3k\n",
    "!pip3 install pandas\n",
    "!pip3 install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnews import GNews\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "google_news = GNews()\n",
    "# google_news.max_results = 5  # number of responses across a keyword\n",
    "google_news.country = 'United States'  # News from a specific country\n",
    "google_news.language = 'english'  # News in a specific language\n",
    "\n",
    "# Set the start and end dates for the 10-year period\n",
    "start_date = datetime(2014, 1, 1)\n",
    "end_date = datetime(2014, 1, 6)\n",
    "\n",
    "errors = []\n",
    "# Open a CSV file to save the results\n",
    "with open('microsoft_news.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['Date', 'Publisher', 'Published_Date', 'Title', 'Description', 'URL', 'Text']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Iterate over each day in the 10-year period\n",
    "    current_date = start_date\n",
    "    \n",
    "    print(f\"Searching Articles in {current_date}\")\n",
    "    google_news.start_date = (current_date.year, current_date.month, current_date.day)\n",
    "    next_date = current_date + timedelta(days=1)\n",
    "    google_news.end_date = (end_date.year, end_date.month, end_date.day)\n",
    "\n",
    "    microsoft_news = google_news.get_news('Microsoft')\n",
    "\n",
    "    # Iterate over the available articles for the current day\n",
    "    for article_info in microsoft_news:\n",
    "        article_data = {\n",
    "            'Date': current_date.strftime('%Y-%m-%d'),\n",
    "            'Publisher': article_info.get('publisher', '').get('title', ''),\n",
    "            'Published_Date': article_info.get('published date', ''),\n",
    "            'Title': article_info.get('title', ''),\n",
    "            'Description': article_info.get('description', ''),\n",
    "            'URL': article_info.get('url', '')\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            article = google_news.get_full_article(article_info['url'])\n",
    "            article_data['Text'] = article.text\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving article: {e}\")\n",
    "            errors.append(e)\n",
    "            continue\n",
    "        writer.writerow(article_data)\n",
    "\n",
    "        \n",
    "\n",
    "print(\"News scraping and CSV export completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Google news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnews import GNews\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def fetch_articles_for_week(start_date, end_date):\n",
    "    print(f\"Fetching Articles from {start_date} to {end_date}\")\n",
    "    local_google_news = GNews()\n",
    "    local_google_news.country = 'United States'\n",
    "    local_google_news.language = 'english'\n",
    "    local_google_news.start_date = (start_date.year, start_date.month, start_date.day)\n",
    "    local_google_news.end_date = (end_date.year, end_date.month, end_date.day)\n",
    "    microsoft_news = local_google_news.get_news('Microsoft')\n",
    "\n",
    "    articles_data = []\n",
    "    for article_info in microsoft_news:\n",
    "        article_data = {\n",
    "            'Date': start_date.strftime('%Y-%m-%d') + \" to \" + end_date.strftime('%Y-%m-%d'),\n",
    "            'Publisher': article_info.get('publisher', '').get('title', ''),\n",
    "            'Published_Date': article_info.get('published date', ''),\n",
    "            'Title': article_info.get('title', ''),\n",
    "            'Description': article_info.get('description', ''),\n",
    "            'URL': article_info.get('url', '')\n",
    "        }\n",
    "        try:\n",
    "            article = local_google_news.get_full_article(article_info['url'])\n",
    "            article_data['Text'] = article.text\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving article for {start_date} to {end_date}: {e}\")\n",
    "            continue\n",
    "        articles_data.append(article_data)\n",
    "    return articles_data\n",
    "\n",
    "def week_ranges(start_date, end_date):\n",
    "    delta = timedelta(days=1)\n",
    "    while start_date < end_date:\n",
    "        week_end = start_date + timedelta(days=6)\n",
    "        if week_end > end_date:\n",
    "            week_end = end_date\n",
    "        yield (start_date, week_end)\n",
    "        start_date += timedelta(days=7)\n",
    "\n",
    "start_date = datetime(2014, 1, 1)\n",
    "end_date = datetime(2024, 1, 1)\n",
    "\n",
    "# Open CSV file for writing results\n",
    "fieldnames = ['Date', 'Publisher', 'Published_Date', 'Title', 'Description', 'URL', 'Text']\n",
    "with open('microsoft_decade_news.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        futures = [executor.submit(fetch_articles_for_week, start, end) for start, end in week_ranges(start_date, end_date)]\n",
    "        for future in as_completed(futures):\n",
    "            articles_data = future.result()\n",
    "            for article_data in articles_data:\n",
    "                writer.writerow(article_data)\n",
    "\n",
    "print(\"Weekly news scraping and CSV export completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "import openai\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "# Set up OpenAI API key\n",
    "OPENAI_API_KEY=\"\"\n",
    "client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
    "# Function to analyze sentiment using OpenAI API\n",
    "encoder = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "def analyze_sentiment(text):\n",
    "    # Define a rough maximum character length based on average token size\n",
    "    max_characters = 16000 * 4  # Assuming an average of 4 characters per token\n",
    "\n",
    "    # Truncate the text if it's longer than the maximum character length\n",
    "    while len(encoder.encode(text)) > 16000:\n",
    "        text = text[:max_characters]\n",
    "        max_characters -= 1000\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        \n",
    "        messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a stock market expert, skilled in analyzing a batch of news for a company and giving a sentiment for its affect on the stock of the company.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"(IMPORTANT! ONLY REPLY WITH A NUMBER)\\nAnalyze the sentiment of the following batch of news articles about Microsoft and provide a rating from 1-100 about its effect on the company's stock (1 being most negative, 100 being most positive). if you give a rating of 100 then that means these news will tremendously affect the stock positively and increase the price. if you give a rating of 0 then that means these news will tremendously affect the stock negatively and decrease the price. If you give a rating of 50 then that means these news will not affect the stock. Here are the articles:\\n\\n{text}\\n\\nSentiment rating (IMPORTANT! ONLY REPLY WITH A NUMBER):\"}\n",
    "            ]\n",
    "        ,\n",
    "        temperature=0.8,\n",
    "        max_tokens=1,\n",
    "        \n",
    "    )\n",
    "    sentiment_rating = response.choices[0].message.content\n",
    "    return sentiment_rating\n",
    "\n",
    "# Read in the news dataset\n",
    "df = pd.read_csv(\"microsoft_decade_news.csv\",  encoding='utf-8')\n",
    "\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Published_Date\"]).dt.date  # Ensure Date is in datetime.date format for accurate grouping\n",
    "\n",
    "df = df.sort_values(\"Date\")\n",
    "# Group by date and select up to 5 articles per date\n",
    "\n",
    "grouped = df.groupby(\"Date\")\n",
    "selected_articles = grouped.apply(lambda x: x.head(10)).reset_index(drop=True)\n",
    "\n",
    "# Open a CSV file to save the results\n",
    "with open('news_sentiment_MSFT.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['Date', 'Sentiment']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Iterate over each day in the 10-year period\n",
    "    for date, group in selected_articles.groupby(\"Date\"):\n",
    "        print(f\"Processing: {date}\")\n",
    "        \n",
    "        # Concatenate 10 articles for the current day\n",
    "        articles_text = '\\n'.join(f\"Article {i}\\nTitle: {row.Title}\\nText: {row.Text}\\n##################################\\n\" for i, row in enumerate(group.itertuples(), 1))\n",
    "\n",
    "        sentiment_rating = analyze_sentiment(articles_text)\n",
    "   \n",
    "        # Write the article details and sentiment rating to the CSV file\n",
    "        writer.writerow({\n",
    "            'Date': date,\n",
    "            'Sentiment': sentiment_rating\n",
    "        })        \n",
    "\n",
    "print(\"News scraping, sentiment analysis, and CSV export completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to calculate cost\n",
    "you have to integrate this with the previous cell to use\n",
    "replace \"analyze_sentiment(articles_text)\" with \"calculate_cost(articles_text)\" and sum the cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import pandas as pd\n",
    "\n",
    "price_token = 0.5\n",
    "encoder = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "def calculate_cost(text):\n",
    "    max_characters = 16000 * 4\n",
    "    while len(encoder.encode(text)) > 16000:\n",
    "        text = text[:max_characters]\n",
    "        max_characters -= 1000\n",
    "    num_tokens = len(encoder.encode(text))\n",
    "    cost = num_tokens / 1000000 * price_token\n",
    "    return cost\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"microsoft_news_sentiment.csv\", encoding=\"utf-8\")\n",
    "\n",
    "df = df.sort_values(\"Date\")\n",
    "\n",
    "df.to_csv(\"news_sentiment_MSFT.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit posts\n",
    "Data downloaded from pushshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv(\"wallstreetbets_submissions.csv\")\n",
    "df2 = pd.read_csv(\"wallstreetbets__submissions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat([df1, df2])\n",
    "df_combined.drop_duplicates(inplace=True)\n",
    "df_combined = df_combined.sort_values(\"created\", ignore_index=True)\n",
    "\n",
    "df_combined.to_csv(\"wallstreetbets.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>created</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u/[deleted]</td>\n",
       "      <td>Earnings season is here.  Place your bets.</td>\n",
       "      <td>13</td>\n",
       "      <td>2012-04-12 00:40</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>I know that /r/investing is a great place for ...</td>\n",
       "      <td>http://www.reddit.com/r/wallstreetbets/comment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u/[deleted]</td>\n",
       "      <td>GOOG - beat estimates, price barely rises.</td>\n",
       "      <td>2</td>\n",
       "      <td>2012-04-13 04:37</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.bloomberg.com/news/2012-04-12/googl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u/[deleted]</td>\n",
       "      <td>My poorly timed opening position for AAPL earn...</td>\n",
       "      <td>12</td>\n",
       "      <td>2012-04-17 06:29</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>So I missed out on GOOG, which is probably a g...</td>\n",
       "      <td>http://www.reddit.com/r/wallstreetbets/comment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u/[deleted]</td>\n",
       "      <td>Anyone betting on VVUS and their potential app...</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-04-17 21:41</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>I'm normally a long, but I've created a second...</td>\n",
       "      <td>http://www.reddit.com/r/wallstreetbets/comment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u/secondhandsondek</td>\n",
       "      <td>EBAY posts higher 1Q net income and revenue, s...</td>\n",
       "      <td>7</td>\n",
       "      <td>2012-04-19 22:10</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://imgur.com/aAfCi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2359721</th>\n",
       "      <td>u/ThtGrlUDntKno</td>\n",
       "      <td>Are small Calls/Puts worth it?</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-01 07:20</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2359722</th>\n",
       "      <td>u/Exegi_One</td>\n",
       "      <td>Future Trading BTC/USDT</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-01 07:28</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://i.redd.it/uvnlbm82tp9c1.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2359723</th>\n",
       "      <td>u/FinanceHubNow</td>\n",
       "      <td>Netflix Gave Director 66,000,000 to Make A Sho...</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-01 07:28</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2359724</th>\n",
       "      <td>u/coco88888</td>\n",
       "      <td>New Year's Eve</td>\n",
       "      <td>211</td>\n",
       "      <td>2024-01-01 07:45</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://i.redd.it/8rbmtol6wp9c1.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2359725</th>\n",
       "      <td>u/acechute</td>\n",
       "      <td>Happy new year 2024!</td>\n",
       "      <td>297</td>\n",
       "      <td>2024-01-01 07:49</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://i.redd.it/aqqq3hpqwp9c1.jpeg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2359726 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     author  \\\n",
       "0               u/[deleted]   \n",
       "1               u/[deleted]   \n",
       "2               u/[deleted]   \n",
       "3               u/[deleted]   \n",
       "4        u/secondhandsondek   \n",
       "...                     ...   \n",
       "2359721     u/ThtGrlUDntKno   \n",
       "2359722         u/Exegi_One   \n",
       "2359723     u/FinanceHubNow   \n",
       "2359724         u/coco88888   \n",
       "2359725          u/acechute   \n",
       "\n",
       "                                                     title  score  \\\n",
       "0               Earnings season is here.  Place your bets.     13   \n",
       "1               GOOG - beat estimates, price barely rises.      2   \n",
       "2        My poorly timed opening position for AAPL earn...     12   \n",
       "3        Anyone betting on VVUS and their potential app...      1   \n",
       "4        EBAY posts higher 1Q net income and revenue, s...      7   \n",
       "...                                                    ...    ...   \n",
       "2359721                     Are small Calls/Puts worth it?      1   \n",
       "2359722                            Future Trading BTC/USDT      1   \n",
       "2359723  Netflix Gave Director 66,000,000 to Make A Sho...      1   \n",
       "2359724                                     New Year's Eve    211   \n",
       "2359725                               Happy new year 2024!    297   \n",
       "\n",
       "                  created                                               link  \\\n",
       "0        2012-04-12 00:40  https://www.reddit.com/r/wallstreetbets/commen...   \n",
       "1        2012-04-13 04:37  https://www.reddit.com/r/wallstreetbets/commen...   \n",
       "2        2012-04-17 06:29  https://www.reddit.com/r/wallstreetbets/commen...   \n",
       "3        2012-04-17 21:41  https://www.reddit.com/r/wallstreetbets/commen...   \n",
       "4        2012-04-19 22:10  https://www.reddit.com/r/wallstreetbets/commen...   \n",
       "...                   ...                                                ...   \n",
       "2359721  2024-01-01 07:20  https://www.reddit.com/r/wallstreetbets/commen...   \n",
       "2359722  2024-01-01 07:28  https://www.reddit.com/r/wallstreetbets/commen...   \n",
       "2359723  2024-01-01 07:28  https://www.reddit.com/r/wallstreetbets/commen...   \n",
       "2359724  2024-01-01 07:45  https://www.reddit.com/r/wallstreetbets/commen...   \n",
       "2359725  2024-01-01 07:49  https://www.reddit.com/r/wallstreetbets/commen...   \n",
       "\n",
       "                                                      text  \\\n",
       "0        I know that /r/investing is a great place for ...   \n",
       "1                                                      NaN   \n",
       "2        So I missed out on GOOG, which is probably a g...   \n",
       "3        I'm normally a long, but I've created a second...   \n",
       "4                                                      NaN   \n",
       "...                                                    ...   \n",
       "2359721                                          [removed]   \n",
       "2359722                                                NaN   \n",
       "2359723                                          [removed]   \n",
       "2359724                                                NaN   \n",
       "2359725                                                NaN   \n",
       "\n",
       "                                                       url  \n",
       "0        http://www.reddit.com/r/wallstreetbets/comment...  \n",
       "1        http://www.bloomberg.com/news/2012-04-12/googl...  \n",
       "2        http://www.reddit.com/r/wallstreetbets/comment...  \n",
       "3        http://www.reddit.com/r/wallstreetbets/comment...  \n",
       "4                                   http://imgur.com/aAfCi  \n",
       "...                                                    ...  \n",
       "2359721  https://www.reddit.com/r/wallstreetbets/commen...  \n",
       "2359722               https://i.redd.it/uvnlbm82tp9c1.jpeg  \n",
       "2359723  https://www.reddit.com/r/wallstreetbets/commen...  \n",
       "2359724               https://i.redd.it/8rbmtol6wp9c1.jpeg  \n",
       "2359725               https://i.redd.it/aqqq3hpqwp9c1.jpeg  \n",
       "\n",
       "[2359726 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(\"wallstreetbets.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where 'text' or 'title' contain 'microsoft' or 'msft'\n",
    "filtered_df = df[(df['text'].str.contains('microsoft|msft', case=False)) | \n",
    "                 (df['title'].str.contains('microsoft|msft', case=False))]\n",
    "filtered_df.to_csv(\"wallstreetbets_MSFT_Microsoft_filtered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Reddit sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "import openai\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "# Set up OpenAI API key\n",
    "OPENAI_API_KEY=\"\"\n",
    "client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
    "# Function to analyze sentiment using OpenAI API\n",
    "encoder = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "def analyze_sentiment(text):\n",
    "    # Define a rough maximum character length based on average token size\n",
    "    max_characters = 16000 * 4  # Assuming an average of 4 characters per token\n",
    "\n",
    "    # Truncate the text if it's longer than the maximum character length\n",
    "    while len(encoder.encode(text)) > 16000:\n",
    "        text = text[:max_characters]\n",
    "        max_characters -= 1000\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a stock market expert, skilled in analyzing a batch of social media posts on reddit and giving a sentiment for its affect on the stock of the company.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"(IMPORTANT! ONLY REPLY WITH A NUMBER)\\nAnalyze the sentiment of the following batch of Reddit posts about Microsoft and provide a rating from 1-100 about its effect on the company's stock (1 being most negative, 100 being most positive). if you give a rating of 100 then that means these posts will tremendously affect the stock positively and increase the price. if you give a rating of 0 then that means these posts will tremendously affect the stock negatively and decrease the price. If you give a rating of 50 then that means these posts will not affect the stock. Here are the posts:\\n\\n{text}\\n\\nSentiment rating (IMPORTANT! ONLY REPLY WITH A NUMBER):\"}\n",
    "            ]\n",
    "        ,\n",
    "        temperature=0.8,\n",
    "        max_tokens=1,\n",
    "        \n",
    "    )\n",
    "    sentiment_rating = response.choices[0].message.content\n",
    "    return sentiment_rating\n",
    "\n",
    "# Read in the reddit dataset\n",
    "df = pd.read_csv(\"wallstreetbets_MSFT_Microsoft_filtered.csv\")\n",
    "\n",
    "df[\"Date\"] = pd.to_datetime(df[\"created\"]).dt.date  # Ensure Date is in datetime.date format for accurate grouping\n",
    "# Filter to keep only dates from 2014-2023\n",
    "df = df[df[\"Date\"] >= pd.to_datetime(\"2014-01-01\").date()]\n",
    "df = df[df[\"Date\"] < pd.to_datetime(\"2024-01-01\").date()]\n",
    "df = df.sort_values(\"Date\")\n",
    "\n",
    "# Group by date and select up to 5 articles per date\n",
    "\n",
    "grouped = df.groupby(\"Date\")\n",
    "selected_articles = grouped.apply(lambda x: x.head(1000)).reset_index(drop=True)\n",
    "\n",
    "# Open a CSV file to save the results\n",
    "with open('microsoft_reddit_sentiment.csv', 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
    "    fieldnames = ['Date', 'Sentiment']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Iterate over each day in the 10-year period\n",
    "    for date, group in selected_articles.groupby(\"Date\"):\n",
    "        print(f\"Processing: {date}\")\n",
    "        \n",
    "        # Concatenate the posts for the current day\n",
    "        posts = '\\n'.join(f\"Article {i}\\nUpvotes: {row.score}\\nTitle: {row.title}\\nText: {row.text}\\n##################################\\n\" for i, row in enumerate(group.itertuples(), 1))\n",
    "\n",
    "        sentiment_rating = analyze_sentiment(posts)\n",
    "   \n",
    "        # Write the article details and sentiment rating to the CSV file\n",
    "        writer.writerow({\n",
    "            'Date': date,\n",
    "            'Sentiment': sentiment_rating\n",
    "        })        \n",
    "\n",
    "print(\"Reddit sentiment analysis, and CSV export completed.\")\n",
    "# sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"microsoft_reddit_sentiment.csv\", encoding=\"utf-8\")\n",
    "\n",
    "df = df.sort_values(\"Date\")\n",
    "df.to_csv(\"Reddit_sentiment_MSFT.csv\", encoding=\"utf-8\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
