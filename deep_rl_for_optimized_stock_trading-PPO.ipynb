{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "929b7fea",
   "metadata": {},
   "source": [
    "# Stock Trading Using Deep Reinforcement Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8989d93c",
   "metadata": {},
   "source": [
    "## Cleaning and Combining the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "717b36c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e909f1",
   "metadata": {},
   "source": [
    "### Defining the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "de816097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import Env \n",
    "from gymnasium import spaces \n",
    "import numpy as np \n",
    "import enum\n",
    "import pandas as pd\n",
    "\n",
    "class Actions(enum.Enum):\n",
    "    Hold = 0\n",
    "    Buy = 1\n",
    "    Sell = 2\n",
    "\n",
    "class StockTradingEnv(Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, df, price_column='Close', sample_window=45, initial_balance=100000, commission_rate=0.001, norm_constant=1):\n",
    "        super(StockTradingEnv, self).__init__()\n",
    "\n",
    "        self.norm_constant = norm_constant          # used to reverse normalization on close price\n",
    "        self.price_column = price_column            # Column to calculate price on\n",
    "        self.sample_window = sample_window          # Window to look at when making a decision, 35 corresponds to 5 days in 30-minute chart\n",
    "        self.initial_balance = initial_balance      # Initial balance in portfolio\n",
    "        self.commission_rate = commission_rate      # Comission rate that the broker takes\n",
    "        self.df = df                                # dataframe\n",
    "        self.current_step = self.sample_window      # Current step the state is at\n",
    "\n",
    "        # Action space consisting of three actions buy, sell, and hold each with a percentage like buy using x% of portfolio\n",
    "        self.action_space = spaces.Box(low=np.array([0, 0]), high=np.array([2, 1]), dtype=np.float32)  \n",
    "        \n",
    "        # \n",
    "        # What state values can be observed, can change the low and high to 0 and 1 if normalization is done\n",
    "        self.observation_space = spaces.Box(low=-np.inf, \n",
    "                                                high=np.inf, \n",
    "                                                shape=(self.df.shape[1] * self.sample_window + 4,),  # the 4 more observations are available cash, share value, action, and percentage\n",
    "                                                dtype=np.float32)\n",
    "        \n",
    "        self.reward_range = (-np.inf, np.inf) # Possible reward range\n",
    "\n",
    "        self.cash = self.initial_balance # Cash is the available money to use to buy stocks\n",
    "        self.shares = 0     # Represents amount of shares in the portfolio for this stock\n",
    "\n",
    "    def reset(self,seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_step = self.sample_window\n",
    "        self.cash = self.initial_balance\n",
    "        self.shares = 0\n",
    "        return self.next_observation([Actions.Hold.value, 0]), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        balance = self.current_balance\n",
    "\n",
    "        self.take_action(action)\n",
    "        obs = self.next_observation(action)\n",
    "        \n",
    "        reward = self.current_balance - balance\n",
    "        done = self.current_step >= self.df.shape[0] - 1\n",
    "        info = { 'Reward' : round(reward, 2),\n",
    "                 'Action' : Actions(round(action[0])).name,\n",
    "                 'Percentage': round(action[1], 2),\n",
    "                 'Shares' : self.shares, \n",
    "                 'Close'  : round(self.current_close_price, 2),\n",
    "                 'Cash'   : round(self.cash, 2), \n",
    "                 'Total'  : round(self.current_balance, 2) }\n",
    "\n",
    "        if not done:\n",
    "            self.current_step += 1\n",
    "        else:\n",
    "            self.reset()\n",
    "\n",
    "        return obs, reward, done, False, info\n",
    "    \n",
    "    def take_action(self, action):\n",
    "        if round(action[0]) == Actions.Buy.value:\n",
    "            if (self.shares == 0):\n",
    "                price = self.current_close_price * (1 + self.commission_rate)\n",
    "                self.shares = action[1] * self.cash / price\n",
    "                self.cash -= self.shares * price\n",
    "        elif round(action[0]) == Actions.Sell.value:\n",
    "            if (self.shares > 0):\n",
    "                price = self.current_close_price * (1 - self.commission_rate)\n",
    "                self.cash += action[1] * self.shares * price\n",
    "                self.shares = 0\n",
    "\n",
    "    def next_observation(self, action):\n",
    "        observation = []\n",
    "        for i in range(self.sample_window, 0, -1):\n",
    "            observation = np.append(observation, self.df.iloc[self.current_step - i])\n",
    "        observation = np.append(observation, [self.cash, self.shares * self.current_close_price, action[0], action[1]])\n",
    "        return observation.astype(np.float32)\n",
    "    \n",
    "    @property\n",
    "    def current_close_price(self):\n",
    "        price = self.df.loc[self.current_step, self.price_column] * self.norm_constant\n",
    "        if np.isnan(price):\n",
    "            raise ValueError(f\"Encountered NaN price at step {self.current_step}\")\n",
    "        return price\n",
    "    \n",
    "    @property\n",
    "    def current_balance(self):\n",
    "        return self.cash + (self.shares * self.current_close_price)\n",
    "\n",
    "### needed modifications: adjust reward function to hate losing more than just winning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a52b06d4-29b6-468b-94cf-2a50d3aa60a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 328  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 6    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 284           |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 14            |\n",
      "|    total_timesteps      | 4096          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00026845478 |\n",
      "|    clip_fraction        | 0.000146      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.84         |\n",
      "|    explained_variance   | -1.74e-05     |\n",
      "|    learning_rate        | 0.0001        |\n",
      "|    loss                 | 1.94e+07      |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -9.81e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.19e+08      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 273          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 22           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047469744 |\n",
      "|    clip_fraction        | 0.0419       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | -0.0869      |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | -0.0123      |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00634     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.00757      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 266         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006246665 |\n",
      "|    clip_fraction        | 0.032       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.83       |\n",
      "|    explained_variance   | 0.237       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0337     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00613    |\n",
      "|    std                  | 0.996       |\n",
      "|    value_loss           | 0.00222     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 260         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 39          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004838021 |\n",
      "|    clip_fraction        | 0.042       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.83       |\n",
      "|    explained_variance   | 0.747       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0388     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00565    |\n",
      "|    std                  | 0.997       |\n",
      "|    value_loss           | 0.00164     |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "import torch as th\n",
    "\n",
    "df = pd.read_csv(\"norm_reduced_data.csv\")\n",
    "\n",
    "# Create the environment\n",
    "env = StockTradingEnv(df)\n",
    "\n",
    "hyperparameters = {\n",
    "    'learning_rate': 0.0001,\n",
    "    'n_steps': 2048,\n",
    "    'batch_size': 64,\n",
    "    'n_epochs': 10,\n",
    "    'gamma': 0.99,\n",
    "    'gae_lambda': 0.95,\n",
    "    'clip_range': 0.2,\n",
    "    'ent_coef': 0.01,\n",
    "    'max_grad_norm': 0.5\n",
    "}\n",
    "\n",
    "# policy_kwargs = dict(activation_fn=th.nn.ReLU, net_arch=[128, 128, 128])\n",
    "\n",
    "# Instantiate the PPO agent\n",
    "policy_kwargs = dict(activation_fn=th.nn.Tanh, net_arch=[64, 64])\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, policy_kwargs=policy_kwargs, **hyperparameters)\n",
    "\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"ppo_stock_trading\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86397123-45a0-4979-adee-e34ce2b84ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = -186040.14049955452\n",
      "Episode 2: Total Reward = -79899.62497822676\n",
      "Episode 3: Total Reward = -88869.81423101001\n",
      "Episode 4: Total Reward = -104810.29758118838\n",
      "Episode 5: Total Reward = -263887.987748681\n",
      "Episode 6: Total Reward = -184421.67849277172\n",
      "Episode 7: Total Reward = -103143.00723997173\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Load the trained model\n",
    "model = PPO.load(\"ppo_stock_trading\")\n",
    "\n",
    "# Number of episodes to test\n",
    "num_episodes = 10\n",
    "\n",
    "# List to store the total rewards for each episode\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Reset the environment and the cumulative reward for the episode\n",
    "    obs, _ = env.reset()\n",
    "    total_rewards = 0\n",
    "    \n",
    "    # Run the episode\n",
    "    while True:\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, reward, done, _, info = env.step(action)\n",
    "        \n",
    "        total_rewards += reward\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Store the total rewards for the episode\n",
    "    episode_rewards.append(total_rewards)\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_rewards}\")\n",
    "\n",
    "    model.save(\"final_trained_model\")\n",
    "\n",
    "# Calculate and print the average reward\n",
    "average_reward = sum(episode_rewards) / num_episodes\n",
    "print(f\"Average Reward: {average_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a20accd-b0bb-49ac-a761-08e461f81e04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
